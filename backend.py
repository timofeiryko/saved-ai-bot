import os
import sys
import aiofiles
from aiocsv import AsyncWriter
import datetime
import logging
import random
from typing import Optional
from dataclasses import dataclass
import polars as pl

from langchain_openai import OpenAIEmbeddings
from langchain_pinecone.vectorstores import Pinecone
from langchain_community.document_loaders import TextLoader, PDFMinerLoader, CSVLoader, PolarsDataFrameLoader
from langchain.docstore.document import Document
from langchain_text_splitters import CharacterTextSplitter
from polars import DataFrame

from langchain.agents.openai_assistant import OpenAIAssistantRunnable
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain.tools import Tool
from langchain.agents import AgentExecutor
from langchain_community.tools import DuckDuckGoSearchRun


from models import TelegramUser
from generate_schema import init
from tortoise import Tortoise

import dotenv
import csv
dotenv.load_dotenv()

@dataclass
class AssistantMessage:
    text: str
    thread_id: str

ASSISTANT_PROMPT = '''
You are a helpful assistant that helps users with their notes. User can search his knowledge base, chat with the assistant, and add new notes.
The assistant should be able to understand the context of the conversation and provide relevant responses.
You are an assistant for question-answering tasks. If you don't know the answer, say that you don't know. Answer like a human, be concise.

Here is the list of comands in telegram bot:
"/search üîé - Your notes\n"
"/chat üí¨ - With the knowledge base\n"
"/note ‚úçÔ∏è - Back to notes mode\n"
"/import üì• - Import notes from Telegram\n"
"/link üîó - Invite friends with discount\n"
"/subscribe ‚úÖ - Your access to the bot\n"
"/update üîÑ - Update the vector store\n"
"/help ‚ÑπÔ∏è - What can I do?"
Here thie info about the bot:
"‚ú® <b>–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ Saved AI!</b> ‚ú®\n\n"
"–ü—Ä–µ–¥—Å—Ç–∞–≤—å —Å–µ–±–µ –ª–∏—á–Ω–æ–≥–æ –ø–æ–º–æ—â–Ω–∏–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –≤—Å–µ –ø—Ä–∏—Å–ª–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –∏—â–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é "
"–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–µ–±–µ –æ–±—â–∞—Ç—å—Å—è —Å —Ç–≤–æ–µ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –≤ –ª—é–±–æ–µ –≤—Ä–µ–º—è.\n\n"
"üìö <b>–§—É–Ω–∫—Ü–∏–∏:</b>\n"
"- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–º–µ—Ç–æ–∫\n"
"- –ì–∏–±–∫–∏–π –ø–æ–∏—Å–∫\n"
"- –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ª—é–±—ã–µ –≤–æ–ø—Ä–æ—Å—ã\n\n"
"üîí <b>480 —Ä / –º–µ—Å, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –±–æ—Ç—É:\n</b> /subscribe"

–Ø–∑—ã–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - —Ä—É—Å—Å–∫–∏–π, –Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±–æ–π —è–∑—ã–∫ –¥–ª—è –∑–∞–º–µ—Ç–æ–∫ –∏ –æ–±—â–µ–Ω–∏—è —Å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º, –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –ø–æ–Ω–∏–º–∞—Ç—å –∏ –æ—Ç–≤–µ—á–∞—Ç—å —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å.

Below is the context, including found relevant documents.
'''

ASSISTANT_PROMPT_RAG = '''
You are an assistant for question-answering tasks. You should take into account text of the message and also this meta info to understand who sent the message:
- From the chat: it's a name of the dialoge the user imported (ignore it unless user asks about the particular imported chat)
- Sender: it's the person who sent the message, take attention to the sender, this is IMPORTANT information
- Forwarded from: if the message was forwarded, it's the original sender
If these fields are not provided, assume that the message is sent by the user, it's his own note.

–Ø–∑—ã–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - —Ä—É—Å—Å–∫–∏–π, –Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±–æ–π —è–∑—ã–∫ –¥–ª—è –∑–∞–º–µ—Ç–æ–∫ –∏ –æ–±—â–µ–Ω–∏—è —Å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º, –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –ø–æ–Ω–∏–º–∞—Ç—å –∏ –æ—Ç–≤–µ—á–∞—Ç—å —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å.

{context}
'''

RAG_PROMPT = ChatPromptTemplate.from_messages(
    [
        ("system", ASSISTANT_PROMPT_RAG),
        ("human", "{input}"),
    ]
)

LLM = ChatOpenAI(model="gpt-4o-mini", temperature=0.6)

INDEX_NAMES = (
    'saved-ai-1',
    'saved-ai-2',
    'saved-ai-3'
)

EMBEDDINGS = OpenAIEmbeddings(model='text-embedding-3-small')

async def handle_event(event_text: str):
    """
    Custom function to handle events or deadlines mentioned by the user.
    Extracts the datetime object from the event_text and prints it.
    """
    try:
        # Attempt to parse the datetime from the event_text
        event_datetime = datetime.datetime.fromisoformat(event_text)
        print(f"Event datetime extracted: {event_datetime}")
    except ValueError:
        # If parsing fails, just print the event_text
        print(f"Could not parse datetime from event: {event_text}")

# Define the custom tool
event_tool = Tool(
    name="EventHandler",
    func=handle_event,
    description="Handles any events or deadlines mentioned by the user by extracting and printing the datetime."
)

TOOLS = []

async def fetch_stats(index_name: str, namespace: str):
    # TODO: Implement this function to fetch stats from Pinecone
    pass

async def start_kb_chat(user: TelegramUser, message: str):

    vector_store = Pinecone.from_existing_index(
        index_name=user.index_name,
        namespace=user.vector_storage_namespace,
        embedding=EMBEDDINGS
    )

    retriever = vector_store.as_retriever()

    question_answer_chain = create_stuff_documents_chain(LLM, RAG_PROMPT)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    result = await rag_chain.ainvoke({'input': message})

    agent = await OpenAIAssistantRunnable.acreate_assistant(
        name='notes-ai-pincone',
        instructions=ASSISTANT_PROMPT,
        model='gpt-4o-mini',
        tools=TOOLS,
        as_agent=True
    )

    relevant_docs = result.get('context', [])
    relevant_contents = list(set([doc.page_content for doc in relevant_docs]))
    relevant_contents.append(result.get('answer', ''))
    relevant_contents.append('Carefully analyze relevant documents provided above and just say "OK"')
    context = "\n\n".join([pc for pc in relevant_contents])
    agent_executor = AgentExecutor(agent=agent, tools=TOOLS)
    response = await agent_executor.ainvoke({'content': context})
    thread_id = response['thread_id']

    return result, thread_id, agent.assistant_id

async def continue_kb_chat(user: TelegramUser, message: str, thread_id, assistant_id):

    agent = OpenAIAssistantRunnable(assistant_id=assistant_id, as_agent=True)
    agent_executor = AgentExecutor(agent=agent, tools=TOOLS)
    response = await agent_executor.ainvoke({'content': message, 'thread_id': thread_id})
    thread_id = response['thread_id']
    output = response['output']

    return output, thread_id, assistant_id


async def generate_csv_from_notes(user: TelegramUser) -> str:
    
    notes = await user.notes.filter(is_vectorized=False).all()
    notes_data = [["message_id", "text"]] + [[note.telegram_message_id, note.text] for note in notes]

    # Generate filename with timestamp
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"{user.telegram_id}_{timestamp}.csv"

    # Asynchronously write CSV content using aiocsv.AsyncWriter
    async with aiofiles.open(filename, 'w', encoding='utf-8', newline='') as f:
        writer = AsyncWriter(f, quotechar='"', quoting=csv.QUOTE_MINIMAL)
        await writer.writerows(notes_data)

    return filename

async def get_docs_from_not_uploaded_notes(user: TelegramUser) -> list[Document]:

    notes = await user.notes.filter(is_vectorized=False).all()
    notes_data = [{"message_id": note.telegram_message_id, "text": note.text} for note in notes]

    docs = []
    for note in notes_data:
        docs.append(Document(
            page_content=note['text'],
            metadata={"source": note['message_id']}
        ))

    return docs

async def upload_notes_to_pinecone(user: TelegramUser):

    documents = await get_docs_from_not_uploaded_notes(user)

    text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=256)
    docs = text_splitter.split_documents(documents)

    if user.index_name:
        index_name = user.index_name
    else:
        index_name = random.choice(INDEX_NAMES)
        user.index_name = index_name
        await user.save()

    vector_store = await Pinecone.afrom_documents(
        docs,
        index_name=index_name,
        embedding=EMBEDDINGS,
        namespace=user.vector_storage_namespace,
    )

    # Mark notes as vectorized
    await user.notes.filter(is_vectorized=False).update(is_vectorized=True)

    return

async def search_notes(user: TelegramUser, query: str):

    vector_store = Pinecone(
        index_name=user.index_name,
        namespace=user.vector_storage_namespace,
        embedding=EMBEDDINGS
    )

    search_results = await vector_store.asimilarity_search_with_relevance_scores(query=query, k=5)
    search_results = [(doc, score) for doc, score in search_results if score > 0.6]
    
    # Sort by relevance score, return only docs
    search_results = [doc for doc, score in sorted(search_results, key=lambda x: x[1], reverse=True)]

    # Remove duplicates
    unique_search_results = []
    seen_sources = set()
    for doc in search_results:
        if doc.metadata['source'] not in seen_sources:
            unique_search_results.append(doc)
            seen_sources.add(doc.metadata['source'])

    # Update user's queries_count
    user.queries_count += 1
    await user.save()

    return unique_search_results

async def upload_exported_chat_to_pinecone(user: TelegramUser, df: DataFrame, chat_name: str):

    # Create column "text", which is msg_content + \n\n + chat_name
    df = df.with_columns(
        (pl.col('msg_content') + f'\n\nFrom the chat: {chat_name}' + '\n\nSender: ' + pl.col('sender') + '\n\nForwarded from: ' + pl.col('forwarded_from')).alias('text')
    )
    df = df.select(['text', 'date'])

    loader = PolarsDataFrameLoader(df, page_content_column='text')
    documents = loader.load()

    text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=256)
    docs = text_splitter.split_documents(documents)

    if user.index_name:
        index_name = user.index_name
    else:
        index_name = random.choice(INDEX_NAMES)
        user.index_name = index_name
        await user.save()

    vector_store = await Pinecone.afrom_documents(
        docs,
        index_name=index_name,
        embedding=EMBEDDINGS,
        namespace=user.vector_storage_namespace
    )

    return